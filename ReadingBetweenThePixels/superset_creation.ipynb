{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "97867df3",
   "metadata": {},
   "source": [
    "Autor@ Félix Fautsch\n",
    "### 'Superset' Dataset Ver1.0: 14.05.2025\n",
    "Creation of a superset dataset consisting of GAHD(2024), HASOC(2019), Bretschneider(2017), IWG(2016), RP-Mod & RP-Crowd(2021) and HOCON34k(2025).\n",
    "Initial superset (no HOCON34k) has been forked from Huggingface: https://huggingface.co/datasets/manueltonneau/german-hate-speech-superset\n",
    "\n",
    "Notes: In future versions maybe remove RP-Crowd (As of 16.05.2025 RP-Mod & RP-Crowd has been completely removed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a03ec7fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import fsspec\n",
    "import re\n",
    "from bs4 import BeautifulSoup\n",
    "from sklearn.model_selection import train_test_split\n",
    "import emoji\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support, fbeta_score, matthews_corrcoef\n",
    "import joblib"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28269b93",
   "metadata": {},
   "source": [
    "#### Functions for cleaning the Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "534a7f5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_split_words(text):\n",
    "    def fix_word(word):\n",
    "        if re.fullmatch(r'(?:[a-zA-Z][\\W_]{0,2}){2,}[a-zA-Z]', word):\n",
    "            return re.sub(r'[\\W_]+', '', word)\n",
    "        else:\n",
    "            return word\n",
    "\n",
    "    words = text.split()\n",
    "    words = [fix_word(word) for word in words]\n",
    "    return ' '.join(words)\n",
    "\n",
    "def remove_emojis(text):\n",
    "    return emoji.replace_emoji(text, replace='')\n",
    "\n",
    "def clean_text(text):\n",
    "    text = BeautifulSoup(text, \"html.parser\").get_text()\n",
    "    text = re.sub(r'http\\S+|www\\S+', '', text)\n",
    "    text = re.sub(r'@\\w+', '', text)\n",
    "    text = re.sub(r'#', '', text)\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    text = remove_emojis(text)\n",
    "    text = normalize_split_words(text)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a11979f0",
   "metadata": {},
   "source": [
    "#### Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f1a75ccd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/pt/3chdc99x3d153mrfqq4_blt40000gn/T/ipykernel_17718/3356785460.py:16: MarkupResemblesLocatorWarning: The input looks more like a filename than markup. You may want to open this file and pass the filehandle into Beautiful Soup.\n",
      "  text = BeautifulSoup(text, \"html.parser\").get_text()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                text  label\n",
      "0       gleich an die wand stellen und erschiessen..      1\n",
      "1  nicht dass ich der Grundbotschaft dieses Posts...      0\n",
      "2  Das mit dem \"an die Wand stellen und erschiessen\"      0\n",
      "3  Seit dem \"an die Wand stellen und erschiessen\"...      0\n",
      "4  Ja ja die Kriminelle Heimatpartei FPÖ von Kind...      0\n",
      "   newspaper_id    post_id  annotator_id  phase split_all split_12  \\\n",
      "0             6  463609874            34      2       NaN      NaN   \n",
      "1             6  463609874            35      2       NaN      NaN   \n",
      "2             6  463609874            36      2       NaN      NaN   \n",
      "3             6  463609874            37      2       NaN      NaN   \n",
      "4             6  463609874            38      2       NaN      NaN   \n",
      "\n",
      "                         label  label_hs  label_context  \\\n",
      "0  Hatespeech (enough context)         1              1   \n",
      "1  Hatespeech (enough context)         1              1   \n",
      "2  Hatespeech (enough context)         1              1   \n",
      "3  Hatespeech (enough context)         1              1   \n",
      "4  Hatespeech (enough context)         1              1   \n",
      "\n",
      "                                                text  \n",
      "0  Mein Motto: Leben und leben lassen ! Aber bei ...  \n",
      "1  Mein Motto: Leben und leben lassen ! Aber bei ...  \n",
      "2  Mein Motto: Leben und leben lassen ! Aber bei ...  \n",
      "3  Mein Motto: Leben und leben lassen ! Aber bei ...  \n",
      "4  Mein Motto: Leben und leben lassen ! Aber bei ...  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/pt/3chdc99x3d153mrfqq4_blt40000gn/T/ipykernel_17718/3356785460.py:16: MarkupResemblesLocatorWarning: The input looks more like a filename than markup. You may want to open this file and pass the filehandle into Beautiful Soup.\n",
      "  text = BeautifulSoup(text, \"html.parser\").get_text()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------------------------------------------------------------ \n",
      "Datasets after cleaning\n",
      "                                                text  label\n",
      "0       gleich an die wand stellen und erschiessen..      1\n",
      "1  nicht dass ich der Grundbotschaft dieses Posts...      0\n",
      "2  Das mit dem \"an die Wand stellen und erschiessen\"      0\n",
      "3  Seit dem \"an die Wand stellen und erschiessen\"...      0\n",
      "4  Ja ja die Kriminelle Heimatpartei FPÖ von Kind...      0\n",
      "                                                text  label split_all\n",
      "0  Das würde ich auch befürworten. Jedochhier geh...      0     train\n",
      "1  Das finde ich auch. Je kleiner das Hirn, desto...      1     train\n",
      "2  Was wäre daran schlimmdiese Subjekte ins Gulag...      1     train\n",
      "3      irgentwas ist dran, daß Löwen a Katzenart is.      0     train\n",
      "4  Nicht die Schwachmaten und Namenstänzer von Li...      0     train\n",
      "------------------------------------------------------------------------------------------ \n",
      "Datasets after splitting\n",
      "                                                text  label split_all\n",
      "0    Dank der verträumten Politiker hahahahahahaahah      0     train\n",
      "1  «Vielleicht war es der zehnminütige Mittagssch...      0     train\n",
      "2  Diese Dame hat einen dicken Bauch, sie sieht k...      1     train\n",
      "3                             Es ist alles gesagt! ?      0     train\n",
      "4  Wie die Merkel da sitzt. Und sowas vertritt De...      0     train\n",
      "                                                text  label split_all\n",
      "0  Das würde ich auch befürworten. Jedochhier geh...      0     train\n",
      "1  Das finde ich auch. Je kleiner das Hirn, desto...      1     train\n",
      "2  Was wäre daran schlimmdiese Subjekte ins Gulag...      1     train\n",
      "3      irgentwas ist dran, daß Löwen a Katzenart is.      0     train\n",
      "4  Nicht die Schwachmaten und Namenstänzer von Li...      0     train\n",
      "int64\n",
      "[0 1]\n",
      "------------------------------------------------------------------------------------------ \n",
      "Superset\n",
      "                                                text  label\n",
      "0    Dank der verträumten Politiker hahahahahahaahah      0\n",
      "1  «Vielleicht war es der zehnminütige Mittagssch...      0\n",
      "2  Diese Dame hat einen dicken Bauch, sie sieht k...      1\n",
      "3                             Es ist alles gesagt! ?      0\n",
      "4  Wie die Merkel da sitzt. Und sowas vertritt De...      0\n",
      "------------------------------------------------------------------------------------------ \n",
      "label\n",
      "0    45270\n",
      "1    10665\n",
      "Name: count, dtype: int64\n",
      "------------------------------------------------------------------------------------------ \n",
      "Balanced Distribution:\n",
      "label\n",
      "0    18108\n",
      "1    10665\n",
      "Name: count, dtype: int64\n",
      "------------------------------------------------------------------------------------------ \n",
      "Balanced Train Distribution:\n",
      "label\n",
      "0    0.628117\n",
      "1    0.371883\n",
      "Name: proportion, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# ------- Huggingface Dataset -------\n",
    "path = \"hf://datasets/manueltonneau/german-hate-speech-superset/de_hf_112024.csv\" # Huggingface Token needed\n",
    "with fsspec.open(path, mode=\"rt\") as f:\n",
    "    df_hf = pd.read_csv(f)\n",
    "df_hf = df_hf[df_hf['dataset'] != 'RP-mod-crowd'] # eliminating RP-mod-crowd dataset\n",
    "df_hf = df_hf.dropna(subset=[\"labels\"])\n",
    "df_hf[\"labels\"] = df_hf[\"labels\"].astype(int)\n",
    "df_hf = df_hf.rename(columns={\"labels\": \"label\"})\n",
    "df_hf = df_hf[[\"text\", \"label\"]]\n",
    "df_hf[\"text\"] = df_hf[\"text\"].apply(clean_text)\n",
    "\n",
    "\n",
    "# ------- HOCON34k Dataset -------\n",
    "df_hocon34k = pd.read_csv(\"/Users/felixfautsch/VS_Python/Projektstudium/hatespeech_hocon34k.csv\")\n",
    "print(df_hf.head())\n",
    "print(df_hocon34k.head())\n",
    "df_hocon34k = df_hocon34k[[\"text\", \"label_hs\", \"split_all\"]]\n",
    "df_hocon34k = df_hocon34k.rename(columns={\"label_hs\": \"label\"})\n",
    "df_hocon34k[\"text\"] = df_hocon34k[\"text\"].apply(clean_text)\n",
    "df_hocon34k = df_hocon34k[df_hocon34k[\"split_all\"].isin([\"train\", \"val\", \"test\"])]\n",
    "df_hocon34k = df_hocon34k.reset_index(drop=True)\n",
    "print(\"-\" * 90,\"\\nDatasets after cleaning\")\n",
    "print(df_hf.head())\n",
    "print(df_hocon34k.head())\n",
    "\n",
    "\n",
    "# ------- Splitting Huggingface Dataset -------\n",
    "df_hf = df_hf.sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "train_df, temp_df = train_test_split(df_hf, test_size=0.2, random_state=42)\n",
    "val_df, test_df = train_test_split(temp_df, test_size=0.5, random_state=42)\n",
    "train_df[\"split_all\"] = \"train\"\n",
    "val_df[\"split_all\"] = \"val\"\n",
    "test_df[\"split_all\"] = \"test\"\n",
    "df_hf = pd.concat([train_df, val_df, test_df], ignore_index=True)\n",
    "print(\"-\" * 90,\"\\nDatasets after splitting\")\n",
    "print(df_hf.head())\n",
    "print(df_hocon34k.head())\n",
    "\n",
    "\n",
    "# ------- Combining both datasets -------\n",
    "df_superset = pd.concat([df_hf,df_hocon34k], ignore_index=True).reset_index(drop=True)\n",
    "df_superset[\"text\"] = df_superset[\"text\"].astype(str)\n",
    "df_superset[\"label\"] = df_superset[\"label\"].astype(int)  \n",
    "df_superset = df_superset.drop(columns=[\"split_all\"])\n",
    "print(df_superset[\"label\"].dtype)\n",
    "print(pd.Series(df_superset[\"label\"]).unique())\n",
    "print(\"-\" * 90,\"\\nSuperset\")\n",
    "print(df_superset.head())\n",
    "print(\"-\" * 90,f\"\\n{df_superset[\"label\"].value_counts().sort_index()}\")\n",
    "\n",
    "\n",
    "# ------- Saving the Superset -------\n",
    "#df_superset.to_csv(\"/Users/felixfautsch/VS_Python/Projektstudium/superset.csv\", index=False) \n",
    "\n",
    "\n",
    "# ------- Balanced Dataset -------\n",
    "df_hatespeech = df_superset[df_superset[\"label\"] == 1][[\"text\", \"label\"]] \n",
    "df_nohatespeech = df_superset[df_superset[\"label\"] == 0].sample(frac=0.4, random_state=42) # 29.295 (with RP-mod-crowd) | 18108 (without RP-mod-crowd)\n",
    "df_balanced = pd.concat([df_hatespeech, df_nohatespeech]).sample(frac=1, random_state=42)\n",
    "print(\"-\" * 90,f\"\\nBalanced Distribution:\\n{df_balanced[\"label\"].value_counts().sort_index()}\")\n",
    "train_df, temp_df = train_test_split(df_balanced, test_size=0.2, random_state=42)\n",
    "val_df, test_df = train_test_split(temp_df, test_size=0.5, random_state=42)\n",
    "print(\"-\" * 90,f\"\\nBalanced Train Distribution:\\n{train_df[\"label\"].value_counts(normalize=True)}\")\n",
    "\n",
    "\n",
    "# ------- Saving the balanced Dataset -------\n",
    "#df_balanced.to_csv(\"/Users/felixfautsch/VS_Python/Projektstudium/balanced.csv\", index=False) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86ac0678",
   "metadata": {},
   "source": [
    "#### Quick Logistic Regression with n-gram(1,3) vectorization\n",
    "S-Score of 0.7468997106112016 on the HOCON34k split_all testset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "86480684",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Danke für Info. Sollte die Redaktion von RO24 auch lesen und entsprechend journalistisch \"reagieren\" und das Foto jetzt nicht verwenden um nicht falsche Eindrücke von den Regierenden zu vermitteln. Gehört sich nicht in Bayern und auch woanders.'\n",
      " 'Egal, was die Leute sagen, ich weigere mich zu akzeptieren, dass schwarze Affen den Weißen gleichgestellt sind'\n",
      " 'Das ist Achmed. Achmed ist IstanbulTerrorist und Mehrfachmörder. Früher war Ahmed Flüchtling in Österreich. NurSo LINK'\n",
      " ...\n",
      " 'Das ist richtig, nur was ist Ihr Schluss daraus? Afghanen nach Österreich oder Belgien abschieben? Das DublinAbkommen war eine Farce und hat u.a. auch Deutschland ein Alibi gegeben, die Länder an der Außengrenze im Stich zu lassen. Es hat nicht mehr funktioniert, als die Länder an den Außengrenzen (nachvollziehbarerweise) nicht mehr alles alleine schultern wollten.'\n",
      " 'Und wieder hat es nichts mit Otte zu tun, bestimmt zum 20 Mal. Sie sind nichts weiter als ein Troll-Clown, der unfähig ist, in der Sache zu diskutieren und sich stattdessen als Kleiklugscheierchen geriert. Zu mehr reicht es nicht. Wie es bei den ganzen rechten Gurkentruppen und ihren Lohnschreibern üblich ist.'\n",
      " '“Arregui betreibt offensichtlich Staatsterrorismus, denn ich möchte wissen, ob die Herstellung von Zigaretten legal oder illegal ist, denn damit Geldwäsche stattfinden kann, muss es eine illegale Aktivität geben.']\n",
      "[0 1 1 ... 0 1 0]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/felixfautsch/VS_Python/.venv/lib/python3.12/site-packages/sklearn/linear_model/_logistic.py:1256: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. Use OneVsRestClassifier(LogisticRegression(..)) instead. Leave it to its default value to avoid this warning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'accuracy': 0.6934306569343066, 'precision': 0.5796862097440132, 'recall': 0.6530232558139535, 'f1': 0.6141732283464567, 'f2': 0.6369080021774632, 'mcc': 0.3631029724077295, 'mcc_normalized': 0.6815514862038647, 'S': 0.659229744190664}\n",
      "{'accuracy': 0.6883252258512856, 'precision': 0.5557418273260687, 'recall': 0.6436893203883495, 'f1': 0.5964912280701754, 'f2': 0.6239412761151891, 'mcc': 0.34728592946714726, 'mcc_normalized': 0.6736429647335737, 'S': 0.6487921204243814}\n",
      "------------------------------------------------------------------------------------------ \n",
      "Baseline Test:\n",
      "{'accuracy': 0.8247151621384751, 'precision': 0.46010362694300516, 'recall': 0.8489483747609943, 'f1': 0.5967741935483871, 'f2': 0.7262021589793916, 'mcc': 0.5351945244860234, 'mcc_normalized': 0.7675972622430117, 'S': 0.7468997106112016}\n",
      "[0]\n",
      "Prediction: 0\n",
      "Probability: [0.6956111 0.3043889]\n"
     ]
    }
   ],
   "source": [
    "train_numpy = np.array(train_df)\n",
    "m, n = train_numpy.shape # m=23018(texts) n=2(labels)\n",
    "print(train_numpy[:,0]) # all rows from column 0\n",
    "print(train_numpy[:,1]) # all rows from column 1\n",
    "texts_train = train_numpy[:,0] # texts\n",
    "labels_train = train_numpy[:,1] # labels\n",
    "labels_train = labels_train.astype(int)\n",
    "\n",
    "val_numpy = np.array(val_df)\n",
    "m, n = train_numpy.shape \n",
    "texts_val = val_numpy[:,0]\n",
    "labels_val = val_numpy[:,1]\n",
    "labels_val = labels_val.astype(int)\n",
    "\n",
    "test_numpy = np.array(test_df)\n",
    "m, n = test_numpy.shape \n",
    "texts_test = test_numpy[:,0]\n",
    "labels_test = test_numpy[:,1]\n",
    "labels_test = labels_test.astype(int)\n",
    "\n",
    "vectorizer = TfidfVectorizer(ngram_range=(1, 3))\n",
    "vectors_train = vectorizer.fit_transform(texts_train)\n",
    "vectors_val = vectorizer.transform(texts_val)\n",
    "vectors_test = vectorizer.transform(texts_test)\n",
    "\n",
    "def compute_metrics(labels, preds):\n",
    "    accuracy = accuracy_score(labels, preds)\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(labels, preds, average='binary')\n",
    "    f2 = fbeta_score(labels, preds, beta=2, average='binary')\n",
    "    mcc = matthews_corrcoef(labels, preds)\n",
    "    mcc_normalized = (mcc + 1) / 2\n",
    "    S = (f2 + mcc_normalized) / 2\n",
    "\n",
    "    metrics = {\n",
    "        \"accuracy\": accuracy,\n",
    "        \"precision\": precision,\n",
    "        \"recall\": recall,\n",
    "        \"f1\": f1,\n",
    "        \"f2\": f2,\n",
    "        \"mcc\": mcc,\n",
    "        \"mcc_normalized\": mcc_normalized,\n",
    "        \"S\": S,\n",
    "    }\n",
    "\n",
    "    return metrics\n",
    "\n",
    "model = LogisticRegression(max_iter=10000,class_weight='balanced',multi_class='ovr')\n",
    "model.fit(vectors_train, labels_train)\n",
    "joblib.dump(model, 'balanced_superset_logreg.pkl')\n",
    "\n",
    "y_pred = model.predict(vectors_val)\n",
    "#print(classification_report(labels_val, y_pred))\n",
    "metrics_val = compute_metrics(labels_val, y_pred)\n",
    "print(metrics_val)\n",
    "loaded_model = joblib.load('balanced_superset_logreg.pkl')\n",
    "y_pred = loaded_model.predict(vectors_test)\n",
    "metrics_test = compute_metrics(labels_test, y_pred)\n",
    "print(metrics_test)\n",
    "\n",
    "\n",
    "# ------- baseline test (on standard hocon34k test split_all) -------\n",
    "baseline_test = df_hocon34k[df_hocon34k[\"split_all\"].isin([\"test\"])]\n",
    "baseline_test = baseline_test.drop(columns=\"split_all\")\n",
    "baseline_numpy = np.array(baseline_test)\n",
    "texts_baseline = baseline_numpy[:,0]\n",
    "labels_baseline = baseline_numpy[:,1]\n",
    "labels_baseline = labels_baseline.astype(int)\n",
    "vectors_baseline = vectorizer.transform(texts_baseline)\n",
    "y_pred = loaded_model.predict(vectors_baseline)\n",
    "metrics_test = compute_metrics(labels_baseline, y_pred)\n",
    "print(\"-\" * 90,f\"\\nBaseline Test:\\n{metrics_test}\")\n",
    "satz = \"Das Wetter ist heute wirklich schön.\"\n",
    "satz_vector = vectorizer.transform([satz])  # needs to be wrapped inside of a list\n",
    "print(loaded_model.predict(satz_vector))\n",
    "prob = loaded_model.predict_proba(satz_vector)\n",
    "print(\"Prediction:\", loaded_model.predict(satz_vector)[0])\n",
    "print(\"Probability:\", prob[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37a12051",
   "metadata": {},
   "source": [
    "#### Analysis of the logistic regression model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6eb86ee0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.02029426 -0.05669088 -0.05669088 ... -0.04630341 -0.04630341\n",
      "  -0.04630341]]\n",
      "(1, 693043)\n",
      "link: -9.3790\n",
      "frauen: 6.2110\n",
      "sind: 4.7105\n",
      "du: 4.1511\n",
      "ausländer: 4.1418\n",
      "schwarze: 3.7550\n",
      "juden: 3.3737\n",
      "sollten: 3.2935\n",
      "menschen: 3.2200\n",
      "muslime: 3.1012\n",
      "schwarzen: 2.9493\n",
      "alle: 2.7297\n",
      "einwanderer: 2.6533\n",
      "diese: 2.6332\n",
      "pack: 2.6191\n",
      "das: -2.6056\n",
      "auch: -2.5703\n",
      "sie: 2.5313\n",
      "lügenpresse: 2.3744\n",
      "würde: 2.3639\n",
      "[-0.34516065]\n",
      "[0 1]\n",
      "{'C': 1.0, 'class_weight': 'balanced', 'dual': False, 'fit_intercept': True, 'intercept_scaling': 1, 'l1_ratio': None, 'max_iter': 10000, 'multi_class': 'ovr', 'n_jobs': None, 'penalty': 'l2', 'random_state': None, 'solver': 'lbfgs', 'tol': 0.0001, 'verbose': 0, 'warm_start': False}\n"
     ]
    }
   ],
   "source": [
    "print(model.coef_) # (classes, weights) weights for every feature, in this case there are 305653 x terms and corresponding weights\n",
    "print(model.coef_.shape) \n",
    "\n",
    "feature_names = vectorizer.get_feature_names_out()\n",
    "coefficients = model.coef_[0]\n",
    "top_features = sorted(zip(coefficients, feature_names), key=lambda x: abs(x[0]), reverse=True)\n",
    "for coef, feat in top_features[:20]:\n",
    "    print(f\"{feat}: {coef:.4f}\")\n",
    "\n",
    "print(model.intercept_)\n",
    "\n",
    "print(model.classes_)  \n",
    "\n",
    "print(model.get_params())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f56d2fc",
   "metadata": {},
   "source": [
    "#### simple BERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07294757",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-german-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/Users/felixfautsch/VS_Python/.venv/lib/python3.12/site-packages/transformers/optimization.py:640: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "Training Epoch 1:   0%|          | 1/1439 [00:40<16:04:36, 40.25s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[7]\u001b[39m\u001b[32m, line 108\u001b[39m\n\u001b[32m    106\u001b[39m total_loss = \u001b[32m0\u001b[39m\n\u001b[32m    107\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m batch \u001b[38;5;129;01min\u001b[39;00m tqdm(train_loader, desc=\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mTraining Epoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch+\u001b[32m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m):\n\u001b[32m--> \u001b[39m\u001b[32m108\u001b[39m     \u001b[43moptimizer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mzero_grad\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    109\u001b[39m     input_ids = batch[\u001b[33m'\u001b[39m\u001b[33minput_ids\u001b[39m\u001b[33m'\u001b[39m].to(device)\n\u001b[32m    110\u001b[39m     attention_mask = batch[\u001b[33m'\u001b[39m\u001b[33mattention_mask\u001b[39m\u001b[33m'\u001b[39m].to(device)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/VS_Python/.venv/lib/python3.12/site-packages/torch/_compile.py:24\u001b[39m, in \u001b[36m_disable_dynamo.<locals>.inner\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m     20\u001b[39m \u001b[38;5;129m@functools\u001b[39m.wraps(fn)\n\u001b[32m     21\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34minner\u001b[39m(*args, **kwargs):\n\u001b[32m     22\u001b[39m     \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_dynamo\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m24\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_dynamo\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdisable\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrecursive\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/VS_Python/.venv/lib/python3.12/site-packages/torch/_dynamo/eval_frame.py:489\u001b[39m, in \u001b[36m_TorchDynamoContext.__call__.<locals>._fn\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    487\u001b[39m     dynamo_config_ctx.\u001b[34m__enter__\u001b[39m()\n\u001b[32m    488\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m489\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    490\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    491\u001b[39m     set_eval_frame(prior)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/VS_Python/.venv/lib/python3.12/site-packages/torch/optim/optimizer.py:820\u001b[39m, in \u001b[36mOptimizer.zero_grad\u001b[39m\u001b[34m(self, set_to_none)\u001b[39m\n\u001b[32m    818\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m p.grad \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    819\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m set_to_none:\n\u001b[32m--> \u001b[39m\u001b[32m820\u001b[39m         p.grad = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    821\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    822\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m p.grad.grad_fn \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "from transformers import BertTokenizer, BertForSequenceClassification\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch\n",
    "from torch.optim import AdamW\n",
    "from torch.nn import functional as F\n",
    "from tqdm import tqdm\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support, fbeta_score, matthews_corrcoef\n",
    "\n",
    "\n",
    "MODEL_NAME = 'bert-base-german-cased'\n",
    "\n",
    "\n",
    "# ------- Dataset -------\n",
    "tokenizer = BertTokenizer.from_pretrained(MODEL_NAME)\n",
    "\n",
    "class TextDataset(Dataset):\n",
    "    def __init__(self, texts, labels, tokenizer, max_len=512):\n",
    "        self.texts = texts\n",
    "        self.labels = labels\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_len = max_len\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        text = str(self.texts[idx])\n",
    "        label = int(self.labels[idx])\n",
    "        encoding = self.tokenizer.encode_plus(\n",
    "            text,\n",
    "            add_special_tokens=True,\n",
    "            max_length=self.max_len,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            return_tensors='pt',\n",
    "            return_attention_mask=True\n",
    "        )\n",
    "\n",
    "        return {\n",
    "            'input_ids': encoding['input_ids'].squeeze(0),\n",
    "            'attention_mask': encoding['attention_mask'].squeeze(0),\n",
    "            'labels': torch.tensor(label, dtype=torch.long)\n",
    "        }\n",
    "\n",
    "train_dataset = TextDataset(texts_train, labels_train, tokenizer)\n",
    "val_dataset = TextDataset(texts_val, labels_val, tokenizer)\n",
    "test_dataset = TextDataset(texts_test, labels_test, tokenizer)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=16)\n",
    "test_loader = DataLoader(test_dataset, batch_size=16)\n",
    "\n",
    "\n",
    "# ------- Training -------\n",
    "model = BertForSequenceClassification.from_pretrained(MODEL_NAME, num_labels=2)\n",
    "\n",
    "def compute_metrics(labels, preds):\n",
    "    accuracy = accuracy_score(labels, preds)\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(labels, preds, average='binary')\n",
    "    f2 = fbeta_score(labels, preds, beta=2, average='binary')\n",
    "    mcc = matthews_corrcoef(labels, preds)\n",
    "    mcc_normalized = (mcc + 1) / 2\n",
    "    S = (f2 + mcc_normalized) / 2\n",
    "\n",
    "    metrics = {\n",
    "        \"accuracy\": accuracy,\n",
    "        \"precision\": precision,\n",
    "        \"recall\": recall,\n",
    "        \"f1\": f1,\n",
    "        \"f2\": f2,\n",
    "        \"mcc\": mcc,\n",
    "        \"mcc_normalized\": mcc_normalized,\n",
    "        \"S\": S,\n",
    "    }\n",
    "\n",
    "    return metrics\n",
    "\n",
    "def evaluate(model, data_loader, device):\n",
    "    model.eval()\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(data_loader, desc=\"Evaluating\"):\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            labels = batch['labels'].to(device)\n",
    "\n",
    "            outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "            logits = outputs.logits\n",
    "            preds = torch.argmax(logits, dim=1)\n",
    "\n",
    "            all_preds.extend(preds.cpu().numpy())\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "    metrics = compute_metrics(all_labels, all_preds)\n",
    "    return metrics\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model.to(device)\n",
    "\n",
    "optimizer = AdamW(model.parameters(), lr=2e-5)\n",
    "\n",
    "epochs = 3\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for batch in tqdm(train_loader, desc=f\"Training Epoch {epoch+1}\"):\n",
    "        optimizer.zero_grad()\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        labels = batch['labels'].to(device)\n",
    "\n",
    "        outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n",
    "        loss = outputs.loss\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    avg_loss = total_loss / len(train_loader)\n",
    "    print(f\"\\nEpoch {epoch+1} | Train Loss: {avg_loss:.4f}\")\n",
    "\n",
    "    val_metrics = evaluate(model, val_loader, device)\n",
    "    print(f\"Validation Metrics: {val_metrics}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
